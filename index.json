[{"authors":null,"categories":null,"content":"Bio Ajay‚Äôs research is that of a full-stack roboticist ‚Äì with a focus on robust, optimal, and agile control + planning for various robots and robotic teams. Current focus is on scalable and learnt multi-robot coordination.\n","date":1672876800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672876800,"objectID":"48f97eacc0dfe9001fbf7d4adf7e3f50","permalink":"https://matteobettini.github.io/authors/shankar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shankar/","section":"authors","summary":"Bio Ajay‚Äôs research is that of a full-stack roboticist ‚Äì with a focus on robust, optimal, and agile control + planning for various robots and robotic teams. Current focus is on scalable and learnt multi-robot coordination.","tags":null,"title":"Ajay Shankar","type":"authors"},{"authors":null,"categories":null,"content":"Bio My research focuses on multi-agent and multi-robot systems. Our mission is to find new ways of coordinating artificially intelligent agents (e.g., robots, vehicles, machines) to achieve common goals in shared physical and virtual spaces.\n","date":1672876800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672876800,"objectID":"19a9369be31a010df83b684612118d3e","permalink":"https://matteobettini.github.io/authors/prorok/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/prorok/","section":"authors","summary":"Bio My research focuses on multi-agent and multi-robot systems. Our mission is to find new ways of coordinating artificially intelligent agents (e.g., robots, vehicles, machines) to achieve common goals in shared physical and virtual spaces.","tags":null,"title":"Amanda Prorok","type":"authors"},{"authors":null,"categories":null,"content":"Hello! üëã I am Matteo, a PhD student in the Prorok Lab at the University of Cambridge. With my supervisor, Amanda Prorok, I study resilience and heterogeneity in multi-agent and multi-robot systems. For my research, I employ techniques from the fields of Multi-Agent Reinforcement Learning and Graph Neural Networks.\nPrior to my PhD, I investigated the problem of transport network design for multi-agent routing.\nDownload my CV.\n","date":1672876800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672876800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matteobettini.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hello! üëã I am Matteo, a PhD student in the Prorok Lab at the University of Cambridge. With my supervisor, Amanda Prorok, I study resilience and heterogeneity in multi-agent and multi-robot systems.","tags":null,"title":"Matteo Bettini","type":"authors"},{"authors":null,"categories":null,"content":"Bio Ryan‚Äôs work focuses on multi-agent reinforcement learning. He is interested in the credit assignment problem, new graph neural network architectures and explainability (applying symbolic regression to multi-agent systems).\n","date":1672790400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672790400,"objectID":"d818a342934a25fd08b803493165c6a5","permalink":"https://matteobettini.github.io/authors/kortvelesy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kortvelesy/","section":"authors","summary":"Bio Ryan‚Äôs work focuses on multi-agent reinforcement learning. He is interested in the credit assignment problem, new graph neural network architectures and explainability (applying symbolic regression to multi-agent systems).","tags":null,"title":"Ryan Kortvelesy","type":"authors"},{"authors":null,"categories":null,"content":"Bio Steven studies how long-term memory can improve decision making in reinforcement learning. He focuses on arranging collections of memories into graph structures, which he queries using graph neural networks. His research aims to improve the reasoning capabilities of robots, allowing them to solve human-level tasks and learn from and correct mistakes in real-time.\n","date":1672790400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672790400,"objectID":"efd495911eaf49f08948ed0d46d0f62a","permalink":"https://matteobettini.github.io/authors/morad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/morad/","section":"authors","summary":"Bio Steven studies how long-term memory can improve decision making in reinforcement learning. He focuses on arranging collections of memories into graph structures, which he queries using graph neural networks. His research aims to improve the reasoning capabilities of robots, allowing them to solve human-level tasks and learn from and correct mistakes in real-time.","tags":null,"title":"Steven Morad","type":"authors"},{"authors":null,"categories":null,"content":"Bio Jan‚Äôs research is about transferring Multi-Agent control policies trained in simulation to the real world (sim-to-real transfer), using Multi-Agent Reinforcement Learning and Graph Neural Networks. He is also interested in interpretability, resilience and robustness of such control policies, particularly in the context of real-world systems.\n","date":1657497600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1657497600,"objectID":"00443489be71f03990762ebf3232d59f","permalink":"https://matteobettini.github.io/authors/blumenkamp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/blumenkamp/","section":"authors","summary":"Bio Jan‚Äôs research is about transferring Multi-Agent control policies trained in simulation to the real world (sim-to-real transfer), using Multi-Agent Reinforcement Learning and Graph Neural Networks. He is also interested in interpretability, resilience and robustness of such control policies, particularly in the context of real-world systems.","tags":null,"title":"Jan Blumenkamp","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Python basics Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Matteo Bettini FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://matteobettini.github.io/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world'] Tuples\nTuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world') Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://matteobettini.github.io/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show() ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://matteobettini.github.io/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://matteobettini.github.io/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://matteobettini.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Matteo Bettini","Ajay Shankar","Amanda Prorok"],"categories":null,"content":"","date":1672876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672876800,"objectID":"96353ba7663707433000bbe313b6614c","permalink":"https://matteobettini.github.io/publication/heterogeneous-multi-robot-reinforcement-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/heterogeneous-multi-robot-reinforcement-learning/","section":"publication","summary":"In this paper, we crystallize the role of heterogeneity in MARL policies. We introduce Heterogeneous Graph Neural Network Proximal Policy Optimization (HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a Graph Neural Network for differentiable inter-agent communication. HetGPPO allows communicating agents to learn heterogeneous behaviors while enabling fully decentralized training in partially observable environments. Through simulations and real-world experiments, we show that\u0026#58; (i) when homogeneous methods fail due to strong heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves higher resilience to both training and deployment noise.","tags":["Heterogeneity","Multi-Agent Reinforcement Learning"],"title":"Heterogeneous Multi-Robot Reinforcement Learning","type":"publication"},{"authors":["Steven Morad","Ryan Kortvelesy","Matteo Bettini","Stephan Liwicki","Amanda Prorok"],"categories":null,"content":"","date":1672790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672790400,"objectID":"ac7347f3a5d5d0c3c93c5e41b44ebe3e","permalink":"https://matteobettini.github.io/publication/popgym-benchmarking-partially-observable-reinforcement-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/popgym-benchmarking-partially-observable-reinforcement-learning/","section":"publication","summary":"We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 14 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines ‚Äì the most in a single RL library. Using POPGym, we execute the largest comparison across RL memory models to date.","tags":null,"title":"POPGym: Benchmarking Partially Observable Reinforcement Learning","type":"publication"},{"authors":["Matteo Bettini","Ryan Kortvelesy","Jan Blumenkamp","Amanda Prorok"],"categories":null,"content":"Scenarios Multi-robot scenarios introduced in VMAS. Robots (blue shapes) interact among each other and with landmarks (green, red, and black shapes) to solve a task. Video ","date":1657497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657497600,"objectID":"5726f0fe709cafafe12ec55b0f739fc4","permalink":"https://matteobettini.github.io/publication/vmas-a-vectorized-multi-agent-simulator-for-collective-robot-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vmas-a-vectorized-multi-agent-simulator-for-collective-robot-learning/","section":"publication","summary":"In this paper, we introduce the Vectorized Multi-Agent Simulator (VMAS). VMAS is an open-source framework designed for efficient MARL benchmarking. It comprises a vectorized 2D physics engine written in PyTorch and a set of twelve challenging multi-robot scenarios. Additional scenarios can be implemented through a simple and modular interface. We demonstrate how vectorization enables parallel simulation on accelerated hardware without added complexity.","tags":["Multi-Agent Reinforcement Learning"],"title":"VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning","type":"publication"},{"authors":["Matteo Bettini","Amanda Prorok"],"categories":null,"content":"","date":1644105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644105600,"objectID":"74794d8f47b6c129c2f2dc3c0e3e2ebd","permalink":"https://matteobettini.github.io/publication/on-the-properties-of-path-additions-for-traffic-routing/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/on-the-properties-of-path-additions-for-traffic-routing/","section":"publication","summary":"In this paper, we investigate the impact of path additions to transport networks with optimised traffic routing. In particular, we study the behaviour of total travel time, and consider both self-interested routing paradigms, such as User Equilibrium (UE) routing, as well as cooperative paradigms, such as classic Multi-Commodity (MC) network flow and System Optimal (SO) routing. This work aims to provide an analysis and categorization of the properties of objective functions for transport network design, with the purpose of informing algorithm (and also network) designers. Among our results, we prove, via counterexample, that total travel time, under both cooperative and self-interested routing, is not supermodular with respect to path additions.","tags":["Transport Network Design"],"title":"On the Properties of Path Additions for Traffic Routing","type":"publication"},{"authors":["Matteo Bettini"],"categories":["Multi-Agent Reinforcement Learning"],"content":" Welcome to VMAS! VMAS is a vectorized framework designed for efficient MARL benchmarking. It comprises a vectorized 2D physics engine written in PyTorch and a set of challenging multi-robot scenarios. Scenario creation is made simple and modular to incentivize contributions. VMAS simulates agents and landmarks of different shapes and supports rotations, elastic collisions, joints, and custom gravity. Holonomic motion models are used for the agents to simplify simulation. Custom sensors such as LIDARs are available and the simulator supports inter-agent communication. Vectorization in PyTorch allows VMAS to perform simulations in a batch, seamlessly scaling to tens of thousands of parallel environments on accelerated hardware. VMAS has an interface compatible with OpenAI Gym and with the RLlib library, enabling out-of-the-box integration with a wide range of RL algorithms. The implementation is inspired by OpenAI\u0026rsquo;s MPE. Alongside VMAS\u0026rsquo;s scenarios, we port and vectorize all the scenarios in MPE.\nPaper The arXiv paper can be found here.\nIf you use VMAS in your research, cite it using:\n@article{bettini2022vmas, title = {VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning}, author = {Bettini, Matteo and Kortvelesy, Ryan and Blumenkamp, Jan and Prorok, Amanda}, year = {2022}, journal={The 16th International Symposium on Distributed Autonomous Robotic Systems}, publisher={Springer} } Video Watch the presentation video of VMAS, showing its structure, scenarios, and experiments.\nTable of contents VectorizedMultiAgentSimulator (VMAS) Welcome to VMAS! Paper Video Table of contents How to use Notebooks Install Run RLlib Simulator features Creating a new scenario Play a scenario Rendering Rendering on server machines List of environments VMAS Main scenarios Debug scenarios MPE TODOS How to use Notebooks Using a VMAS environment. Here is a simple notebook that you can run to create, step and render any scenario in VMAS. It reproduces the use_vmas_env.py script in the examples folder. Using VMAS in RLlib. In this notebook, we show how to use any VMAS scenario in RLlib. It reproduces the rllib.py script in the examples folder.\nInstall To install the simulator, simply install the requirements using:\npip install -r requirements.txt and then install the package with:\npip install -e . Run To use the simulator, simply create an environment by passing the name of the scenario you want (from the scenarios folder) to the make_env function. The function arguments are explained in the documentation. The function returns an environment object with the OpenAI gym interface:\nHere is an example:\nenv = vmas.make_env( scenario_name=\u0026quot;waterfall\u0026quot;, num_envs=32, device=\u0026quot;cpu\u0026quot;, # Or \u0026quot;cuda\u0026quot; for GPU continuous_actions=True, wrapper=None, # One of: None, vmas.Wrapper.RLLIB, and vmas.Wrapper.GYM max_steps=None, # Defines the horizon. None is infinite horizon. **kwargs # Additional arguments you want to pass to the scenario initialization ) A further example that you can run is contained in use_vmas_env.py in the examples directory.\nRLlib To see how to use VMAS in RLlib, check out the script in examples/rllib.py.\nSimulator features Vectorized: VMAS vectorization can step any number of environments in parallel. This significantly reduces the time needed to collect rollouts for training in MARL. Simple: Complex vectorized physics engines exist (e.g., Brax), but they do not scale efficiently when dealing with multiple agents. This defeats the computational speed goal set by vectorization. VMAS uses a simple custom 2D dynamics engine written in PyTorch to provide fast simulation. General: The core of VMAS is structured so that it can be used to implement general high-level multi-robot problems in 2D. It can support adversarial as well as cooperative scenarios. Holonomic point-robot simulation has been chosen to focus on general high-level problems, without learning low-level custom robot controls through MARL. Extensible: VMAS is not just a simulator with a set of environments. It is a framework that can be used to create new multi-agent scenarios in a format that is usable by the whole MARL community. For this purpose, we have modularized the process of creating a task and introduced interactive rendering to debug it. You can define your own scenario in minutes. Have a look at the dedicated section in this document. Compatible: VMAS has wrappers for RLlib and OpenAI Gym. RLlib has a large number of already implemented RL algorithms. Keep in mind that this interface is less efficient than the unwrapped version. For an example of wrapping, see the main of make_env. Tested: Our scenarios come with tests which run a custom designed heuristic on each scenario. Entity shapes: Our entities (agent and landmarks) can have different customizable shapes (spheres, boxes, lines). All these shapes are supported for elastic collisions. Faster than physics engines: Our simulator is extremely lightweight, using only tensor operations. It is perfect for running MARL training at scale with multi-agent collisions and interactions. Customizable: When creating a new scenario of your own, the world, agent and landmarks are highly customizable. Examples are: drag, friction, gravity, simulation timestep, non-differentiable communication, agent sensors (e.g. LIDAR), and masses. Non-differentiable communication: Scenarios can require agents to perform discrete or continuous communication actions. Gravity: VMAS supports customizable gravity. Sensors: Our simulator implements ray casting, which can be used to simulate a wide range of distance-based sensors that can be added to agents. We currently support LIDARs. To see available sensors, have a look at the sensors script. Joints: Our simulator supports joints. Joints are constraints that keep entities at a specified distance. The user can specify the anchor points on the two objects, the distance (including 0), the thickness of the joint, if the joint is allowed to rotate at either anchor point, and if he wants the joint to be collidable. Have a look at the waterfall scenario to see how you can use joints. See the waterfall and joint_passage scenarios for an example. Agent actions: Agents\u0026rsquo; physical actions are 2D forces for holonomic motion. Agent rotation can also be controlled through a torque action (activated by setting agent.action.u_rot_range at agent creation time). Agents can also be equipped with continuous or discrete communication actions. Action preprocessing and velocity controller: By implementing the process_action function of a scenario, you can modify the agents\u0026rsquo; actions before they are passed to the simulator. This can be used to enforce a specific dynamics model (e.g., differential drive). We provide a VelocityController which can be used in this function to treat input actions as velocities (instead of forces). This PID controller takes velocities and outputs the forces which are fed to the simulator. See the vel_control debug scenario for an example. Creating a new scenario To create a new scenario, just extend the BaseScenario class in scenario.py.\nYou will need to implement at least make_world, reset_world_at, observation, and reward. Optionally, you can also implement done, info, process_action, and extra_render.\nYou can also change the viewer size, zoom, and enable a background rendered grid by changing these inherited attributes in the make_world function.\nTo know how, just read the documentation of BaseScenario in scenario.py and look at the implemented scenarios.\nPlay a scenario You can play with a scenario interactively! Just execute its script!\nJust use the render_interactively function in the interactive_rendering.py script. Relevant values will be plotted to screen. Move the agent with the arrow keys and switch agents with TAB. You can reset the environment by pressing R. If you have more than 1 agent, you can control another one with W,A,S,D and switch the second agent using LSHIFT. To do this, just set control_two_agents=True.\nOn the screen you will see some data from the agent controlled with arrow keys. This data includes: name, current obs, current reward, total reward so far and environment done flag.\nHere is an overview of what it looks like:\nRendering To render the environment, just call the render or the try_render_at functions (depending on environment wrapping).\nExample:\nenv.render( mode=\u0026quot;rgb_array\u0026quot;, # \u0026quot;rgb_array\u0026quot; returns image, \u0026quot;human\u0026quot; renders in display agent_index_focus=4, # If None keep all agents in camera, else focus camera on specific agent index=0, # Index of batched environment to render visualize_when_rgb: bool = False, # Also run human visualization when mode==\u0026quot;rgb_array\u0026quot; plot_position_function=None, # A function to plot under the rendering. This function takes the position (x,y) as input and outputs a transparency alpha value. This can be used to visualize a value function. ) You can also change the viewer size, zoom, and enable a background rendered grid by changing these inherited attributes in the scenario make_world function.\nGif Agent focus With agent_index_focus=None the camera keeps focus on all agents With agent_index_focus=0 the camera follows agent 0 With agent_index_focus=4 the camera follows agent 4 Rendering on server machines To render in machines without a display use mode=\u0026quot;rgb_array\u0026quot;. Make sure you have OpenGL and Pyglet installed. To enable rendering on headless machines you should install EGL. If you do not have EGL, you need to create a fake screen. You can do this by running these commands before the script:\nexport DISPLAY=':99.0' Xvfb :99 -screen 0 1400x900x24 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; or in this way:\nxvfb-run -s \\\u0026quot;-screen 0 1400x900x24\\\u0026quot; python \u0026lt;your_script.py\u0026gt; To create a fake screen you need to have Xvfb installed.\nList of environments VMAS dropout\nfootball\ntransport\nwheel\nbalance\nreverse transport\ngive_way\npassage\ndispersion\njoint_passage_size\nflocking\ndiscovery\njoint_passage\nball_passage\nball_trajectory\nbuzz_wire\nmulti_give_way\nMain scenarios Env name Description GIF dropout.py In this scenario, n_agents and a goal are spawned at random positions between -1 and 1. Agents cannot collide with each other and with the goal. The reward is shared among all agents. The team receives a reward of 1 when at least one agent reaches the goal. A penalty is given to the team proportional to the sum of the magnitude of actions of every agent. This penalises agents for moving. The impact of the energy reward can be tuned by setting energy_coeff. The default coefficient is 0.02 makes it so that for one agent it is always worth reaching the goal. The optimal policy consists in agents sending only the closest agent to the goal and thus saving as much energy as possible. Every agent observes its position, velocity, relative position to the goal and a flag that is set when someone reaches the goal. The environment terminates when when someone reaches the goal. To solve this environment, communication is needed. dispersion.py In this scenario, n_agents agents and goals are spawned. All agents spawn in [0,0] and goals spawn at random positions between -1 and 1. Agents cannot collide with each other and with the goals. Agents are tasked with reaching the goals. When a goal is reached, the team gets a reward of 1 if share_reward is true, otherwise the agents which reach that goal in the same step split the reward of 1. If penalise_by_time is true, every agent gets an additional reward of -0.01 at each step. The optimal policy is for agents to disperse and each tackle a different goal. This requires high coordination and diversity. Every agent observes its position and velocity. For every goal it also observes the relative position and a flag indicating if the goal has been already reached by someone or not. The environment terminates when all the goals are reached. transport.py In this scenario, n_agents, n_packages (default 1) and a goal are spawned at random positions between -1 and 1. Packages are boxes with package_mass mass (default 50 times agent mass) and package_width and package_length as sizes. The goal is for agents to push all packages to the goal. When all packages overlap with the goal, the scenario ends. Each agent receives the same reward which is proportional to the sum of the distance variations between the packages and the goal. In other words, pushing a package towards the goal will give a positive reward, while pushing it away, a negative one. Once a package overlaps with the goal, it becomes green and its contribution to the reward becomes 0. Each agent observes its position, velocity, relative position to packages, package velocities, relative positions between packages and the goal and a flag for each package indicating if it is on the goal. By default packages are very heavy and one agent is barely able to push them. Agents need to collaborate and push packages together to be able to move them faster. reverse_transport.py This is exactly the same of transport except with n_agents spawned inside a single package. All the rest is the same. give_way.py In this scenario, two agents and two goals are spawned in a narrow corridor. The agents need to reach the goal with their color. The agents are standing in front of each other\u0026rsquo;s goal and thus need to swap places. In the middle of the corridor there is an asymmetric opening which fits one agent only. Therefore the optimal policy is for one agent to give way to the other. This requires heterogeneous behaviour. Each agent observes its position, velocity and the relative position to its goal. The scenario terminates when both agents reach their goals. wheel.py In this scenario, n_agents are spawned at random positions between -1 and 1. One line with line_length and line_mass is spawned in the middle. The line is constrained in the origin and can rotate. The goal of the agents is to make the absolute angular velocity of the line match desired_velocity. Therefore, it is not sufficient for the agents to all push in the extrema of the line, but they need to organize to achieve, and not exceed, the desired velocity. Each agent observes its position, velocity, the current angle of the line module pi, the absolute difference between the current angular velocity of the line and the desired one, and the relative position to the two line extrema. The reward is shared and it is the absolute difference between the current angular velocity of the line and the desired one. balance.py In this scenario, n_agents are spawned uniformly spaced out under a line upon which lies a spherical package of mass package_mass. The team and the line are spawned at a random X position at the bottom of the environment. The environment has vertical gravity. If random_package_pos_on_line is True (default), the relative X position of the package on the line is random. In the top half of the environment a goal is spawned. The agents have to carry the package to the goal. Each agent receives the same reward which is proportional to the distance variation between the package and the goal. In other words, getting the package closer to the goal will give a positive reward, while moving it away, a negative one. The team receives a negative reward of -10 for making the package or the line fall to the floor. The observations for each agent are: its position, velocity, relative position to the package, relative position to the line, relative position between package and goal, package velocity, line velocity, line angular velocity, and line rotation mod pi. The environment is done either when the package or the line fall or when the package touches the goal. football.py In this scenario, a team of n_blue_agents play football against a team of n_red_agents. The boolean parameters ai_blue_agents and ai_red_agents specify whether each team is controlled by action inputs or a programmed AI. Consequently, football can be treated as either a cooperative or competitive task. The reward in this scenario can be tuned with dense_reward_ratio, where a value of 0 denotes a fully sparse reward (1 for a goal scored, -1 for a goal conceded), and 1 denotes a fully dense reward (based on the the difference of the \u0026ldquo;attacking value\u0026rdquo; of each team, which considers the distance from the ball to the goal and the presence of open dribbling/shooting lanes to the goal). Every agent observes its position, velocity, relative position to the ball, and relative velocity to the ball. The episode terminates when one team scores a goal. discovery.py In this scenario, a team of n_agents has to coordinate to cover n_targets targets as quickly as possible while avoiding collisions. A target is considered covered if agents_per_target agents have approached a target at a distance of at least covering_range. After a target is covered, the agents_per_target each receive a reward and the target is respawned to a new random position. Agents receive a penalty if they collide with each other. Every agent observes its position, velocity, LIDAR range measurements to other agents and targets (independently). The episode terminates after a fixed number of time steps. flocking.py In this scenario, a team of n_agents has to flock around a target while staying together and maximising their velocity without colliding with each other and a number of n_obstacles obstacles. Agents are penalized for colliding with each other and with obstacles, and are rewarded for maximising velocity and minimising the span of the flock (cohesion). Every agent observes its position, velocity, and LIDAR range measurements to other agents. The episode terminates after a fixed number of time steps. passage.py In this scenario, a team of 5 robots is spawned in formation at a random location in the bottom part of the environment. A simular formation of goals is spawned at random in the top part. Each robot has to reach its corresponding goal. In the middle of the environment there is a wall with n_passages. Each passage is large enough to fit one robot at a time. Each agent receives a reward which is proportional to the distance variation between itself and the goal. In other words, getting closer to the goal will give a positive reward, while moving it away, a negative one. This reward will be shared in case shared_reward is true. If collisions among robots occur, each robot involved will get a reward of -10. Each agent observes: its position, velocity, relative position to the goal and relative position to the center of each passage. The environment terminates when all the robots reach their goal. joint_passage_size.py Here, two robots of different sizes (blue circles),connected by a linkage through two revolute joints, need to cross a passage while keeping the linkage parallel to it and then match the desired goal position (green circles) on the other side. The passage is comprised of a bigger and a smaller gap, which are spawned in a random position and order on the wall, but always at the same distance between each other. The team is spawned in a random order and position on the lower side with the linkage always perpendicular to the passage. The goal is spawned horizontally in a random position on the upper side. Each robot observes its velocity, relative position to each gap, and relative position to the goal center. The robots receive a shaped global reward that guides them to the goal without colliding with the passage. joint_passage.py This is the same as joint_passage_size.py with the difference that the robots are now physically identical, but the linkage has an asymmetric mass (black circle). The passage is a single gap, positioned randomly on the wall. The agents need to cross it while keeping the linkage perpendicular to the wall and avoiding collisions. The team and the goal are spawned in a random position, order, and rotation on opposite sides of the passage. ball_passage.py This is the same as joint_passage.py, except now the agents are not connected by linkages and need to push a ball through the passage. The reward is only dependent on the ball and it\u0026rsquo;s shaped to guide it through the passage. ball_trajectory.py This is the same as circle_trajectory.py except the trajectory reward is now dependent on a ball object. Two agents need to drive the ball in a circular trajectory. If joints=True the agents are connected to the ball with linkages. buzz_wire.py Two agents are connected to a mass through linkages and need to play the Buzz Wire game in a straight corridor. Be careful not to touch the borders, or the episode ends! multi_give_way.py Like give_way, but with four agents. This scenario requires high coordination to be solved. Debug scenarios Env name Description GIF waterfall.py n_agents agents are spawned in the top of the environment. They are all connected to each other through collidable linkages. The last agent is connected to a box. Each agent is rewarded based on how close it is to the center of the black line at the bottom. Agents have to reach the line and in doing so they might collide with each other and with boxes in the environment. asym_joint.py Two agents are connected by a linkage with an asymmetric mass. The agents are rewarded for bringing the linkage to a vertical position while consuming the least team energy possible. vel_control.py Example scenario where three agents have velocity controllers with different acceleration constraints goal.py An agent with a velocity controller is spawned at random in the workspace. It is rewarded for moving to a randomly initialised goal while consuming the least energy. The agent observes its velocity and the relative position to the goal. het_mass.py Two agents with different masses are spawned randomly in the workspace. They are rewarded for maximising the team maximum speed while minimizing the team energy expenditure. The optimal policy requires the heavy agent to stay still while the light agent moves at maximum speed. line_trajectory.py One agent is rewarded to move in a line trajectory . circle_trajectory.py One agent is rewarded to move in a circle trajectory at the desired_radius. MPE Env name in code (name in paper) Communication? Competitive? Notes simple.py N N Single agent sees landmark position, rewarded based on how close it gets to landmark. Not a multi-agent environment \u0026ndash; used for debugging policies. simple_adversary.py (Physical deception) N Y 1 adversary (red), N good agents (green), N landmarks (usually N=2). All agents observe position of landmarks and other agents. One landmark is the ‚Äòtarget landmark‚Äô (colored green). Good agents rewarded based on how close one of them is to the target landmark, but negatively rewarded if the adversary is close to target landmark. Adversary is rewarded based on how close it is to the target, but it doesn‚Äôt know which landmark is the target landmark. So good agents have to learn to ‚Äòsplit up‚Äô and cover all landmarks to deceive the adversary. simple_crypto.py (Covert communication) Y Y Two good agents (alice and bob), one adversary (eve). Alice must sent a private message to bob over a public channel. Alice and bob are rewarded based on how well bob reconstructs the message, but negatively rewarded if eve can reconstruct the message. Alice and bob have a private key (randomly generated at beginning of each episode), which they must learn to use to encrypt the message. simple_push.py (Keep-away) N Y 1 agent, 1 adversary, 1 landmark. Agent is rewarded based on distance to landmark. Adversary is rewarded if it is close to the landmark, and if the agent is far from the landmark. So the adversary learns to push agent away from the landmark. simple_reference.py Y N 2 agents, 3 landmarks of different colors. Each agent wants to get to their target landmark, which is known only by other agent. Reward is collective. So agents have to learn to communicate the goal of the other agent, and navigate to their landmark. This is the same as the simple_speaker_listener scenario where both agents are simultaneous speakers and listeners. simple_speaker_listener.py (Cooperative communication) Y N Same as simple_reference, except one agent is the ‚Äòspeaker‚Äô (gray) that does not move (observes goal of other agent), and other agent is the listener (cannot speak, but must navigate to correct landmark). simple_spread.py (Cooperative navigation) N N N agents, N landmarks. Agents are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with other agents. So, agents have to learn to cover all the landmarks while avoiding collisions. simple_tag.py (Predator-prey) N Y Predator-prey environment. Good agents (green) are faster and want to avoid being hit by adversaries (red). Adversaries are slower and want to hit good agents. Obstacles (large black circles) block the way. simple_world_comm.py Y Y Environment seen in the video accompanying the paper. Same as simple_tag, except (1) there is food (small blue balls) that the good agents are rewarded for being near, (2) we now have ‚Äòforests‚Äô that hide agents inside from being seen from outside; (3) there is a ‚Äòleader adversary‚Äù that can see the agents at all times, and can communicate with the other adversaries to help coordinate the chase. TODOS Talk about action preprocessing and velocity controller New envs from joint project with their descriptions New ens from adversarial project Custom actions for scenario Implement 1D camera sensor Make football heuristic efficient Link video of experiments Add LIDAR section Implement LIDAR Rewrite all MPE scenarios simple simple_adversary simple_crypto simple_push simple_reference simple_speaker_listener simple_spread simple_tag simple_world_comm ","date":1638662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638662400,"objectID":"4374679d307f8097fe028e90ed0e877d","permalink":"https://matteobettini.github.io/post/vmas/","publishdate":"2021-12-05T00:00:00Z","relpermalink":"/post/vmas/","section":"post","summary":"VMAS is a vectorized simulator designed for efficient Multi-Agent Reinforcement Learning benchmarking. It is comprised of a vectorized 2D physics engine written in PyTorch and a set of challenging multi-robot scenarios. Additional scenarios can be implemented through a simple and modular interface.","tags":["Multi-Agent Reinforcement Learning"],"title":"VMAS, Vectorized Multi-Agent Simulator","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://matteobettini.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://matteobettini.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://matteobettini.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://matteobettini.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]